在第10章中，我们讨论了批处理--一种读入一堆文件作为输入，生成一堆新的输出文件的技术。输出是派生数据的形式，意味着，如果有必要可以通过批处理重新创建数据集。我们看到这是多么简单有力的思路在创建搜索索引，推荐分析系统的时候。

但是，存在一个假设：即输入是有界的（大小已知且有限），因此批处理可以知道何时读取完成它的输入。例如，MapReduce的核心排序操作在生产输出前必须获得所有输入：可能会发生最后一个输入是最小键的情况，因此需要将它作为第一个输出，所以不能提前开始输出。

实际上，许多数据是无界的，因为它们会随着时间推移慢慢到达：你的用户昨天和今天都在生产数据，而且明天还会继续。除非你倒闭，这个过程不会停止，从这个意义上来说，数据不会完成。因此批处理程序必须人为将其划分为固定持续时间的块：例如，在每天的最后处理一天的数据，或者在每个小时结束处理一小时的数据。

每日批处理的问题在于输入的更改要在一天之后才反映在输出中，这对于很多用户来说太慢了。为了降低延迟，我们可以更加平滑的运行处理过程----例如，在每秒结束处理一秒的数据----或者甚至，连续地没有所谓的固定时间块，就当时间发生时就处理。这就是流处理背后的思想。

通常来说，"流"是指随着时间推移逐渐可用的数据。这个概念出现在许多地方：Unix的`stdin和stdout`，编程语言（惰性列表），文件系统的API，TCP连接，互联网上的音视频传输等等。

在本章中，我们将事件流视为一种数据管理机制：对应上一章批处理的无界的，增量处理的对应物。首先讨论如何通过网络表示，存储和传输流。在451页的数据库和流中我们将研究流和数据库的关系，最后，在464页的流处理中，我们将探索用于连续处理这些流的方法和工具，以及可用于构建应用程序的方法。

## 传输事件流

在批处理系统，任务的输入和输出都是文件（可能来自分布式文件系统）。等效的流是什么样子？

当输入是文件（字节的序列）时，处理的第一步就是解析为记录序列。在流处理的上下文中，记录通常被成为event，但是本质上是一个东西：小的，自包含的，不变的对象，其中包含了某个时间点发生的某件事的详细信息。event通常包含时间戳标识发生于一天中的那个时间点。

